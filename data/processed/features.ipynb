{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n",
      "[nltk_data] Error loading opinion_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1129)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'word_count': 0 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'char_count': 0 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'avg_word_length': 27 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'positive_word_count': 0 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'negative_word_count': 0 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'product_average_rating': 0 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'product_rating_count': 0 Inf-Werte, 0 NaN-Werte\n",
      "Feature 'log_helpful_votes': 0 Inf-Werte, 0 NaN-Werte\n",
      "Anzahl der Zeilen mit word_count == 0: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 810/810 [03:30<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle verarbeiteten Daten und Modelle wurden erfolgreich gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NLTK download just one time\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('opinion_lexicon')\n",
    "\n",
    "# Lemma and Stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, 'v') for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def count_sentiment_words(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    pos_count = sum(1 for word in tokens if word in positive_words)\n",
    "    neg_count = sum(1 for word in tokens if word in negative_words)\n",
    "    return pos_count, neg_count\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"../raw/merged_reviews_metadata_renamed.csv\", sep=\";\")\n",
    "df_clean = df[df['product_main_category'] == 'Musical Instruments'][[\n",
    "    'review_rating', 'review_title', 'review_text', 'helpful_votes',\n",
    "    'is_verified_purchase', 'product_title', 'product_average_rating',\n",
    "    'product_rating_count', 'products_bought_together'\n",
    "]]\n",
    "\n",
    "# Drop rows with missing data\n",
    "df_clean = df_clean.dropna(subset=['review_title', 'review_text'])\n",
    "\n",
    "# Lemma\n",
    "df_clean['review_title_lemmatized'] = df_clean['review_title'].apply(lemmatize_text)\n",
    "df_clean['review_text_lemmatized'] = df_clean['review_text'].apply(lemmatize_text)\n",
    "df_clean['full_review'] = df_clean['review_title_lemmatized'] + ' ' + df_clean['review_text_lemmatized']\n",
    "\n",
    "# Count word and char\n",
    "df_clean['word_count'] = df_clean['full_review'].apply(lambda x: len(x.split()))\n",
    "df_clean['char_count'] = df_clean['full_review'].apply(lambda x: len(x))\n",
    "df_clean['avg_word_length'] = df_clean['char_count'] / df_clean['word_count']\n",
    "\n",
    "# Sentiment features\n",
    "df_clean[['positive_word_count', 'negative_word_count']] = df_clean['full_review'].apply(\n",
    "    lambda x: pd.Series(count_sentiment_words(x))\n",
    ")\n",
    "\n",
    "# Drop products_bought_together (not enough rows)\n",
    "df_clean = df_clean.drop(columns=['products_bought_together'])\n",
    "\n",
    "# Log-Transformation \n",
    "df_clean['log_helpful_votes'] = np.log1p(df_clean['helpful_votes'])\n",
    "\n",
    "numerical_features = [\n",
    "    'word_count', 'char_count', 'avg_word_length', \n",
    "    'positive_word_count', 'negative_word_count',\n",
    "    'product_average_rating', 'product_rating_count', 'log_helpful_votes'\n",
    "]\n",
    "\n",
    "for feature in numerical_features:\n",
    "    num_inf = np.isinf(df_clean[feature]).sum()\n",
    "    num_nan = df_clean[feature].isna().sum()\n",
    "    print(f\"Feature '{feature}': {num_inf} Inf-Werte, {num_nan} NaN-Werte\")\n",
    "\n",
    "# Change INF to NaA\n",
    "df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with word_count == 0\n",
    "zero_word_count = (df_clean['word_count'] == 0).sum()\n",
    "print(f\"Sum word_count == 0: {zero_word_count}\")\n",
    "df_clean = df_clean[df_clean['word_count'] != 0]\n",
    "\n",
    "# Imputation with Med\n",
    "df_clean[numerical_features] = df_clean[numerical_features].fillna(df_clean[numerical_features].median())\n",
    "\n",
    "# Final Check\n",
    "for feature in numerical_features:\n",
    "    if np.isinf(df_clean[feature]).any():\n",
    "        print(f\"Feature '{feature}' Inf-Rows.\")\n",
    "    if df_clean[feature].isna().any():\n",
    "        print(f\"Feature '{feature}' NaN-Rows.\")\n",
    "\n",
    "df_clean['is_verified_purchase'] = df_clean['is_verified_purchase'].astype(int).astype(np.float32)\n",
    "\n",
    "# scaler and float32\n",
    "scaler = StandardScaler()\n",
    "df_clean[numerical_features] = scaler.fit_transform(df_clean[numerical_features]).astype(np.float32)\n",
    "\n",
    "# TF-IDF-Features\n",
    "tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2))  # Reduce max_features (not enough storage)\n",
    "tfidf_features = tfidf.fit_transform(df_clean['full_review']).astype(np.float32)\n",
    "\n",
    "# Sentence Embeddings\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(df_clean['full_review'].tolist(), show_progress_bar=True, convert_to_numpy=True).astype(np.float32)\n",
    "\n",
    "# Convert Embeddings to Sparse-Matrix\n",
    "embeddings_sparse = sparse.csr_matrix(embeddings)\n",
    "\n",
    "# Dimensionality reduction Embeddings to 150 (storage)\n",
    "svd_embeddings = TruncatedSVD(n_components=150, random_state=42)\n",
    "embeddings_reduced = svd_embeddings.fit_transform(embeddings_sparse)\n",
    "embeddings_reduced_sparse = sparse.csr_matrix(embeddings_reduced.astype(np.float32))\n",
    "\n",
    "# Dimensionality reduction TF-IDF-Features to 150 (storage)\n",
    "svd_tfidf = TruncatedSVD(n_components=150, random_state=42)\n",
    "tfidf_reduced = svd_tfidf.fit_transform(tfidf_features)\n",
    "tfidf_reduced_sparse = sparse.csr_matrix(tfidf_reduced.astype(np.float32))\n",
    "\n",
    "# Combine the reduced features\n",
    "numeric_features_values = df_clean[numerical_features].values.astype(np.float32)\n",
    "\n",
    "categorical_features = df_clean['is_verified_purchase'].values.reshape(-1,1).astype(np.float32)\n",
    "\n",
    "# Convert numeric and categorical features to sparse matrix\n",
    "numeric_sparse = sparse.csr_matrix(numeric_features_values)\n",
    "categorical_sparse = sparse.csr_matrix(categorical_features)\n",
    "\n",
    "# Combining the reduced TF-IDF features, reduced embeddings, numerical and categorical features\n",
    "X = sparse.hstack([tfidf_reduced_sparse, embeddings_reduced_sparse, numeric_sparse, categorical_sparse]).astype(np.float32)\n",
    "\n",
    "# Target\n",
    "y = df_clean['review_rating'].astype(np.int32).values \n",
    "\n",
    "# Safe features and target\n",
    "def save_processed_data(X, y, tfidf_vectorizer, svd_tfidf, svd_embeddings, \n",
    "                       X_path='X_features_reduced.joblib', \n",
    "                       y_path='y_target_reduced.joblib', \n",
    "                       tfidf_path='tfidf_vectorizer_reduced.joblib', \n",
    "                       svd_tfidf_path='svd_tfidf_reduced.joblib', \n",
    "                       svd_embeddings_path='svd_embeddings_reduced.joblib'):\n",
    "\n",
    "    # Saving the feature matrix with maximum compression\n",
    "    joblib.dump(X, X_path, compress=9)\n",
    "    \n",
    "    # Saving the target with maximum compression\n",
    "    joblib.dump(y, y_path, compress=9)\n",
    "    \n",
    "    # Saving the tfidf with maximum compression\n",
    "    joblib.dump(tfidf_vectorizer, tfidf_path, compress=9)\n",
    "    \n",
    "    # Saving the SVD with maximum compression\n",
    "    joblib.dump(svd_tfidf, svd_tfidf_path, compress=9)\n",
    "    joblib.dump(svd_embeddings, svd_embeddings_path, compress=9)\n",
    "    \n",
    "    print(\"Alle verarbeiteten Daten und Modelle wurden erfolgreich gespeichert.\")\n",
    "\n",
    "save_processed_data(\n",
    "    X, y, tfidf, svd_tfidf, svd_embeddings,\n",
    "    X_path='X_features_reduced.joblib', \n",
    "    y_path='y_target_reduced.joblib', \n",
    "    tfidf_path='tfidf_vectorizer_reduced.joblib', \n",
    "    svd_tfidf_path='svd_tfidf_reduced.joblib', \n",
    "    svd_embeddings_path='svd_embeddings_reduced.joblib'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
